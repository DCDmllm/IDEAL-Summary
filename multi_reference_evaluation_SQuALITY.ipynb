{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi reference rougel for SQuALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from rouge_score import rouge_scorer\n",
    "import json\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2','rougeL', 'rougeLsum'], use_stemmer=True, split_summaries=True)\n",
    "with open(f'{path}/outputs/LLaMA3-Lora-hyper/SQuALITY/b32_epoch6_warme1_lorar8_loraQ,K,V,O,FFN_UP_nhyper16-32_diffe_parallel_blr6e3_lossOlabelsTrue_maxseq8000_flashattnTrue_/predict_mingen200.jsonl', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "data = [json.loads(line) for line in lines]\n",
    "\n",
    "idset = []\n",
    "new_data = {}\n",
    "for x in data:\n",
    "    if x.get('instruction'):\n",
    "        ins_art = x.get('instruction')+x.get('article')\n",
    "    else:\n",
    "        ins_art = x.get('article')\n",
    "\n",
    "    if ins_art in idset:\n",
    "        x['idx'] = idset.index(ins_art)\n",
    "        new_data[x['idx']].append(x)\n",
    "    else:\n",
    "        idset.append(ins_art)\n",
    "        x['idx'] = len(idset)-1\n",
    "        new_data[x['idx']] = [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rougel(abstract, generate):\n",
    "    generate = generate.replace('<pad>','').replace('<s>','').replace('</s>','')\n",
    "    scores = scorer.score(abstract, generate)\n",
    "    rouge1 = scores['rouge1'].fmeasure\n",
    "    rouge2 = scores['rouge2'].fmeasure\n",
    "    rougeL = scores['rougeL'].fmeasure\n",
    "    rougeLsum = scores['rougeLsum'].fmeasure\n",
    "    return rouge1, rouge2, rougeL, rougeLsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "rouge1, rouge2, rougeL, rougeLsum= 0, 0, 0, 0\n",
    "for k,v in new_data.items():\n",
    "    # print(len(v))\n",
    "    r1_l = []\n",
    "    r2_l = []\n",
    "    rL_l = []\n",
    "    rLsum_l = []\n",
    "    for one in v:\n",
    "        r1, r2, rL, rLsum = rougel(one['abstract'], one['generate'])\n",
    "        r1_l.append(r1)\n",
    "        r2_l.append(r2)\n",
    "        rL_l.append(rL)\n",
    "        rLsum_l.append(rLsum)\n",
    "    max_idx = rL_l.index(max(rL_l))\n",
    "    rouge1 += r1_l[max_idx]\n",
    "    rouge2 += r2_l[max_idx]\n",
    "    rougeL += rL_l[max_idx]\n",
    "    rougeLsum += rLsum_l[max_idx]\n",
    "    # print(rL_l)\n",
    "    # print(rLsum_l)\n",
    "    # print(rougeL)\n",
    "    cnt += 1\n",
    "    # break\n",
    "print(f\"rouge1: {rouge1 / cnt}, rouge2: {rouge2 / cnt}, rougeL: {rougeL / cnt}, rougeLsum: {rougeLsum / cnt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi reference bart_score for SQuALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import BERTScorer\n",
    "\n",
    "Bert_scorer = BERTScorer(model_type=f'{path}/pretrain_models/bart_base', num_layers=6,device='cuda:0',\n",
    "# Bert_scorer = BERTScorer(model_type=f'{path}/pretrain_models/deberta-xlarge-mnli', num_layers=40,device='cuda:0', # is not the bertscore in Qontsum\n",
    "                    batch_size=256,\n",
    "                    nthreads=8,\n",
    "                    # idf=True,\n",
    "                    # rescale_with_baseline=True,\n",
    "                    # lang=\"en\"\n",
    "                    # idf_sents=refs\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_ref = []\n",
    "all_gen = []\n",
    "number_l = []\n",
    "for k,v in new_data.items():\n",
    "    number_l.append(len(v))\n",
    "    for one in v:\n",
    "        all_ref.append(one['abstract'])\n",
    "        all_gen.append(one['generate'].replace('<pad>','').replace('<s>','').replace('</s>',''))\n",
    "\n",
    "len(number_l)\n",
    "\n",
    "_, _, f1 = Bert_scorer.score(all_gen, all_ref)\n",
    "\n",
    "cnt = 0\n",
    "bart_score= 0\n",
    "ind = 0\n",
    "for numb in number_l:\n",
    "    barts_l = list(f1[ind:ind+numb])\n",
    "    max_barts = max(barts_l)\n",
    "    bart_score += max_barts\n",
    "    cnt += 1\n",
    "    ind = ind+numb\n",
    "print(f\"bart_score: {bart_score / cnt}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
